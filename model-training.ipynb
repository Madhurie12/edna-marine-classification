{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11683481,"sourceType":"datasetVersion","datasetId":7332902}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nfrom sklearn.decomposition import PCA\n\n# Step 1: Read the .xlsx file\ndf = pd.read_excel('/kaggle/input/edna/edna new merged data.xlsx', engine='openpyxl')\n\n# Clean column names\ndf.columns = df.columns.str.strip()\nprint(\"Columns in dataset:\", df.columns.tolist())\n\n# Step 2: Check for target column\nif 'scientificName' not in df.columns:\n    raise ValueError(\"Error: 'scientificName' column not found in the dataset.\")\n\n# Define features and target\nexclude_cols = ['scientificName', 'id', 'occurrenceID', 'eventID', 'fieldNumber', \n                'verbatimIdentification', 'pcr_primer_forward', 'pcr_primer_reverse', \n                'pcr_primer_reference', 'eventDate', 'locality']\nX = df.drop(columns=exclude_cols)\ny = df['scientificName']\n\n# Step 3: Filter out rare classes\nmin_class_size = 3\ny_counts = y.value_counts()\nvalid_classes = y_counts[y_counts >= min_class_size].index\nX_filtered = X[y.isin(valid_classes)].copy()\ny_filtered = y[y.isin(valid_classes)]\nprint(\"Original class distribution:\", Counter(y_filtered))\n\n# Step 4: Preprocess features\nnumerical_cols = ['organismQuantity', 'sampleSizeValue', 'decimalLatitude', 'decimalLongitude']\ncategorical_cols = ['env_broad_scale', 'lib_layout', 'target_gene', 'seq_meth', \n                    'pcr_primer_name_forward', 'pcr_primer_name_reverse', 'basisOfRecord', \n                    'organismQuantityType', 'occurrenceStatus', 'sampleSizeUnit', \n                    'country', 'geodeticDatum']\ntext_cols = ['DNA_sequence']\n\n# Handle missing values\nX_filtered.loc[:, numerical_cols] = X_filtered[numerical_cols].fillna(X_filtered[numerical_cols].mean())\nX_filtered.loc[:, categorical_cols] = X_filtered[categorical_cols].fillna('Unknown')\nX_filtered.loc[:, text_cols] = X_filtered[text_cols].fillna('')\n\n# K-mer extraction\ndef extract_kmers(sequence, k=3):\n    sequence = str(sequence) if sequence else \"\"\n    return Counter([sequence[i:i+k] for i in range(max(0, len(sequence)-k+1))])\n\nkmer_features = X_filtered['DNA_sequence'].apply(extract_kmers)\nkmer_df = pd.DataFrame(kmer_features.tolist(), index=X_filtered.index).fillna(0)\n\n# PCA on k-mers\npca = PCA(n_components=50, random_state=42)\nkmer_reduced = pca.fit_transform(kmer_df)\nkmer_columns = [f'kmer_pca_{i}' for i in range(kmer_reduced.shape[1])]\nkmer_df = pd.DataFrame(kmer_reduced, index=kmer_df.index, columns=kmer_columns)\n\n# Combine features\nX_filtered = pd.concat([X_filtered[numerical_cols + categorical_cols], kmer_df], axis=1)\nX_filtered = pd.get_dummies(X_filtered, columns=categorical_cols, drop_first=True)\nfeatures = X_filtered.columns.tolist()\n\n# Step 5: SMOTE\nsmote = SMOTE(random_state=42, k_neighbors=2)\nX_res, y_res = smote.fit_resample(X_filtered, y_filtered)\nprint(\"Resampled class distribution:\", Counter(y_res))\n\n# Step 6: Encode labels\nlabel_encoder = LabelEncoder()\ny_res_encoded = label_encoder.fit_transform(y_res)\n\n# Step 7: Train model\nclf = RandomForestClassifier(class_weight='balanced', max_depth=10, min_samples_split=10, \n                             n_estimators=200, max_features='sqrt', random_state=42)\ncross_val_scores = cross_val_score(clf, X_res, y_res_encoded, cv=5)\nprint(f\"Cross-validation accuracy scores: {cross_val_scores}\")\nprint(f\"Mean cross-validation accuracy: {cross_val_scores.mean()}\")\n\nclf.fit(X_res, y_res_encoded)\ny_pred = clf.predict(X_res)\nprint(\"Classification Report:\")\nprint(classification_report(y_res_encoded, y_pred, target_names=label_encoder.classes_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode target labels\nlabel_encoder = LabelEncoder()\ny_res_encoded = label_encoder.fit_transform(y_res)\n\n# Initialize Gradient Boosting Classifier\nclf = GradientBoostingClassifier(learning_rate=0.1, n_estimators=200, max_depth=3, random_state=42)\n\n# Perform cross-validation\ncross_val_scores = cross_val_score(clf, X_res, y_res_encoded, cv=5)\n\n# Print cross-validation accuracy scores\nprint(f\"Cross-validation accuracy scores: {cross_val_scores}\")\nprint(f\"Mean cross-validation accuracy: {cross_val_scores.mean()}\")\n\n# Train on full data and print classification report\nclf.fit(X_res, y_res_encoded)\ny_pred = clf.predict(X_res)\nprint(\"Classification Report:\")\nprint(classification_report(y_res_encoded, y_pred, target_names=label_encoder.classes_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report\nfrom xgboost import XGBClassifier\n\n# Initialize the LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Encode the target labels into numeric values\ny_encoded = label_encoder.fit_transform(y_res)  # Using y_res after resampling\n\n# Use XGBoost Classifier\nclf = XGBClassifier(\n    objective=\"multi:softmax\",\n    eval_metric=\"mlogloss\",\n    use_label_encoder=False,\n    scale_pos_weight=1,  # Helps with class imbalance\n    random_state=42\n)\n\n# Fit the model\nclf.fit(X_res, y_encoded)\n\n# Make predictions\ny_pred = clf.predict(X_res)\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_encoded, y_pred))\n\n# If you want to see the class labels corresponding to the numeric values:\nprint(\"Class labels:\", label_encoder.classes_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import classification_report\n\n# Use LightGBM Classifier with suppressed verbose output\nclf = LGBMClassifier(\n    class_weight='balanced',  # Helps with class imbalance\n    random_state=42,\n    n_jobs=-1,  # Use all cores to speed up computation\n    boosting_type='gbdt',\n    objective='multiclass',   # For multi-class classification\n    num_class=len(set(y_res)), # Number of unique classes in target\n    verbose=-1  # Suppress training messages\n)\n\n# Fit the model\nclf.fit(X_res, y_res)\n\n# Predict the target values\ny_pred = clf.predict(X_res)\n\n# Print the classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_res, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Use Extra Trees Classifier\nclf = ExtraTreesClassifier(\n    class_weight='balanced',  # Handles class imbalance\n    random_state=42,\n    n_jobs=-1  # Use all cores to speed up computation\n)\n\n# Perform cross-validation\ncross_val_scores = cross_val_score(clf, X_res, y_res, cv=5)\n\n# Print cross-validation accuracy scores\nprint(f\"Cross-validation accuracy scores: {cross_val_scores}\")\nprint(f\"Mean cross-validation accuracy: {cross_val_scores.mean()}\")\n\nclf.fit(X_res, y_res)\ny_pred = clf.predict(X_res)\nprint(\"Classification Report:\")\nprint(classification_report(y_res, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\n\n# Use HistGradientBoosting Classifier\nclf = HistGradientBoostingClassifier(\n    random_state=42,\n    class_weight='balanced'  # Handle class imbalance\n)\n\n# Perform cross-validation\ncross_val_scores = cross_val_score(clf, X_res, y_res, cv=5)\n\n# Print cross-validation accuracy scores\nprint(f\"Cross-validation accuracy scores: {cross_val_scores}\")\nprint(f\"Mean cross-validation accuracy: {cross_val_scores.mean()}\")\nclf.fit(X_res, y_res)\ny_pred = clf.predict(X_res)\nprint(\"Classification Report:\")\nprint(classification_report(y_res, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Use K-Nearest Neighbors Classifier\nclf = KNeighborsClassifier(\n    n_neighbors=5,\n    weights='distance'  # Uses distance-weighted neighbors\n)\n\n# Perform cross-validation\ncross_val_scores = cross_val_score(clf, X_res, y_res, cv=5)\n\n# Print cross-validation accuracy scores\nprint(f\"Cross-validation accuracy scores: {cross_val_scores}\")\nprint(f\"Mean cross-validation accuracy: {cross_val_scores.mean()}\")\n\nclf.fit(X_res, y_res)\ny_pred = clf.predict(X_res)\nprint(\"Classification Report:\")\nprint(classification_report(y_res, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\n# Use Voting Classifier with different models\nclf = VotingClassifier(\n    estimators=[\n        ('lr', LogisticRegression(class_weight='balanced', random_state=42)),\n        ('gb', GradientBoostingClassifier(random_state=42)),\n        ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n    ],\n    voting='soft'\n)\n\n# Perform cross-validation\ncross_val_scores = cross_val_score(clf, X_res, y_res, cv=5)\n\n# Print cross-validation accuracy scores\nprint(f\"Cross-validation accuracy scores: {cross_val_scores}\")\nprint(f\"Mean cross-validation accuracy: {cross_val_scores.mean()}\")\n\nclf.fit(X_res, y_res)\ny_pred = clf.predict(X_res)\nprint(\"Classification Report:\")\nprint(classification_report(y_res, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}